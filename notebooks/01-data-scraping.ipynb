{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "- Scrape images of common grocery items. These include:\n",
    "    - Butter\n",
    "    - Soy Milk\n",
    "    - Soy Sauce\n",
    "    - Rice bags\n",
    "    - Chin Kiang Vinegar\n",
    "    - Yoghurt\n",
    "    - Kecap Manis\n",
    "    - Coconut Water\n",
    "    - Canned Tuna\n",
    "    - Ms Chens Prawn Hargow\n",
    "    \n",
    "I used [this floydhub blog post](https://blog.floydhub.com/web-scraping-with-python/) as a web-scraping guide. Key Ideas:\n",
    "- Jupyter notebooks great for agile development\n",
    "- But, cache results in CSV to avoid needing to re-scrape every time\n",
    "\n",
    "## Surprise...don't scrape Google Images!\n",
    "Reason: https://stackoverflow.com/questions/36438261/extracting-images-from-google-images-using-src-and-beautifulsoup\n",
    "tldr;\n",
    "- Google makes it super difficult\n",
    "- Use Google's [custom search API](https://cse.google.com/cse/create/new) instead\n",
    "- In fact, ALWAYS use an API before scraping!\n",
    "- Http request: GET https://www.googleapis.com/customsearch/v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Google Custom Search Engine to store images\n",
    "import requests\n",
    "import json\n",
    "\n",
    "ROOT_URL = 'https://www.googleapis.com/customsearch/v1?'\n",
    "google_custom_api_key = 'AIzaSyBOzEUMnkNGzrTDCK1USlTyTFovHY95czw' #Bob Jin's private API key\n",
    "SE_ID = '007117080109183320818:lzdpxbparoi' #Custom entire web image search engine\n",
    "product_to_search = 'lurpak'\n",
    "searchType = 'image'\n",
    "\n",
    "q = f'q={product_to_search}'\n",
    "key = f'key={google_custom_api_key}'\n",
    "cx = f'cx={SE_ID}'\n",
    "searchType = f'searchType={searchType}'\n",
    "\n",
    "# Initialise list of returned images and data\n",
    "imgs = []\n",
    "\n",
    "for start in [1,11,21,31,41]:\n",
    "    start = f'start={start}'\n",
    "\n",
    "    params = [q,cx,searchType,start,key]\n",
    "    SUFFIX_URL = '&'.join(params)\n",
    "    url = ROOT_URL + SUFFIX_URL\n",
    "    result = requests.get(url)\n",
    "    \n",
    "    c = json.loads(result.text)\n",
    "    \n",
    "    items = c['items']\n",
    "\n",
    "    for item in items:\n",
    "        img = {\n",
    "            'product': product_to_search,\n",
    "            'link': item['link'],\n",
    "            'height': item['image']['height'],\n",
    "            'width': item['image']['width'],\n",
    "        }\n",
    "        imgs.append(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to Store Images for Later Access\n",
    "https://realpython.com/storing-images-in-python/#storing-many-images\n",
    "tldr;\n",
    "- Use a HDF5 format to improve efficiency\n",
    "- I will have to store my images to the disk anyway\n",
    "- I can save space by augmenting images in memory and saving to a HDF5 database (will need functions)\n",
    "\n",
    "\n",
    "# Augmenting image data\n",
    "Useful Medium blogs:\n",
    "- [Part 1](https://medium.com/nanonets/nanonets-how-to-use-deep-learning-when-you-have-limited-data-f68c0b512cab)\n",
    "- [Part 2](https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'product': 'lurpak',\n",
       " 'link': 'https://cdnprod.mafretailproxy.com/cdn-cgi/image/format=auto,onerror=redirect/sys-master-prod/h52/hc6/9049142919198/662867_main.jpg_480Wx480H',\n",
       " 'height': 480,\n",
       " 'width': 480}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download images and store in data\n",
    "from io import open as iopen\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "def requests_image(file_url,new_file_name=None):\n",
    "#     suffix_list = ['jpg', 'gif', 'png', 'tif', 'svg',]\n",
    "#     file_name =  urlsplit(file_url)[2].split('/')[-1]\n",
    "#     file_suffix = file_name.split('.')[1]\n",
    "    i = requests.get(file_url)\n",
    "    \n",
    "#     if new_file_name is None:\n",
    "#         new_file_name = file_name\n",
    "#     else:\n",
    "#         new_file_name += f'.{file_suffix}'\n",
    "    \n",
    "    if i.status_code == requests.codes.ok:\n",
    "        with iopen(new_file_name, 'wb') as file:\n",
    "            file.write(i.content)\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "img_count = 0\n",
    "\n",
    "for img in imgs:\n",
    "    img_count += 1\n",
    "    img_filename = f'{product_to_search}_{img_count}'\n",
    "    img_path = f'data/{img_filename}.png'\n",
    "    requests_image(img['link'],img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
